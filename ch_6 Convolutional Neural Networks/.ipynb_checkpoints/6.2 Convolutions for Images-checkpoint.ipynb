{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cov layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, kernel):\n",
    "    y_h, y_w = x.shape[0] - kernel.shape[0] + 1, x.shape[1] - kernel.shape[1] + 1\n",
    "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
    "    y = torch.zeros(y_h, y_w)\n",
    "#     pdb.set_trace()\n",
    "    for i in range(y_h):\n",
    "        for j in range(y_w):\n",
    "#             pdb.set_trace()\n",
    "            y[i, j] = (x[i:i+k_h, j:j+k_w] * kernel).sum()\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.Tensor([[0, 1], [2, 3]])\n",
    "conv2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.]]),\n",
       " tensor([[ 1., -1.]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(6,8)\n",
    "X[:, 2:6] = 0\n",
    "K = torch.Tensor([[1, -1]])\n",
    "X, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = conv2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.kernel = torch.randn(kernel_size, dtype=torch.float32, reqiures_grad = True)\n",
    "        self.bias = torch.zeros((1,), dtype=torch.float32, requires_grad = True)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return conv2d(data, self.kernel) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.5154, grad_fn=<SumBackward0>)\n",
      "tensor(6.0156, grad_fn=<SumBackward0>)\n",
      "tensor(3.2953, grad_fn=<SumBackward0>)\n",
      "tensor(1.8818, grad_fn=<SumBackward0>)\n",
      "tensor(1.1113, grad_fn=<SumBackward0>)\n",
      "tensor(0.6731, grad_fn=<SumBackward0>)\n",
      "tensor(0.4152, grad_fn=<SumBackward0>)\n",
      "tensor(0.2593, grad_fn=<SumBackward0>)\n",
      "tensor(0.1633, grad_fn=<SumBackward0>)\n",
      "tensor(0.1035, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(1,1, kernel_size=(1,2), bias=False)\n",
    "\n",
    "x = X.reshape(1,1,6,8)\n",
    "y = Y.reshape(1,1,6,7)\n",
    "\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    out = conv(x)\n",
    "    conv.zero_grad()\n",
    "    loss = ((out - y) ** 2).sum()\n",
    "    loss.backward()\n",
    "    conv.weight.data -= lr * conv.weight.grad\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0248, -0.9590]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1.]]),\n",
       " tensor([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [-1.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0., -1.,  0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0., -1.,  0.,  0.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., -1.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0.,  0., -1.,  0.,  0.]]),\n",
       " tensor([[ 1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.],\n",
       "         [-1.,  0.,  0.,  1.,  0.],\n",
       "         [ 0., -1.,  0.,  0.,  1.],\n",
       "         [ 0.,  0., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  0.,  0.,  0., -1.]]),\n",
       " tensor([[ 1.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0., -1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0., -1.]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.eye(5)\n",
    "x = torch.zeros(6, 8)\n",
    "for i in range(x.shape[0]):\n",
    "    x[i, i:i+3] = 1\n",
    "k = torch.Tensor([[1, -1]])\n",
    "x, conv2d(x, k), conv2d(x.T, k), conv2d(x, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-f79eee77d541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "X = torch.ones(6,8)\n",
    "X[:, 2:6] = 0\n",
    "K = torch.Tensor([[1, -1]])\n",
    "\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    out = conv2d(X, K)\n",
    "#     conv2d.zero_grad()\n",
    "#     pdb.set_trace()\n",
    "    loss = ((out - y) ** 2).sum()\n",
    "    loss.backward()\n",
    "    conv.weight.data -= lr * conv.weight.grad\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l] *",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
