{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 padding\n",
    "\n",
    "2-dim conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_conv2d(pad, input, kernel):\n",
    "    padded = torch.ones(input.shape[0]+pad*2, input.shape[1]+pad*2)\n",
    "    padded[pad:input.shape[0]+pad, pad:input.shape[1]+pad] = input\n",
    "    print(padded.shape)\n",
    "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
    "    out_size = [padded.shape[0]-k_h+1, padded.shape[1]-k_w+1]\n",
    "    output = torch.zeros(out_size, dtype=torch.float32)\n",
    "    \n",
    "    for i in range(out_size[0]):\n",
    "        for j in range(out_size[1]):\n",
    "            output[i, j] = (padded[i:i+k_h, j:j+k_w] * kernel).sum()\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.]]),\n",
       " tensor([[ 1., -1.]]),\n",
       " tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(6,8)\n",
    "X[:, 2:6] = 0\n",
    "K = torch.Tensor([[1, -1]])\n",
    "X, K, pad_conv2d(0, X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4524, 0.1933, 0.7932, 0.3182, 0.6474, 0.9099, 0.7452, 0.7475],\n",
       "         [0.3564, 0.9284, 0.1694, 0.6654, 0.4113, 0.1532, 0.1088, 0.6975],\n",
       "         [0.3212, 0.9623, 0.5122, 0.1980, 0.2479, 0.2753, 0.8209, 0.5198],\n",
       "         [0.3846, 0.5112, 0.6789, 0.8449, 0.7111, 0.4067, 0.9715, 0.8780],\n",
       "         [0.0583, 0.2051, 0.1125, 0.6189, 0.3062, 0.6472, 0.3071, 0.1311],\n",
       "         [0.2598, 0.5291, 0.1472, 0.4688, 0.5587, 0.1194, 0.6290, 0.9241],\n",
       "         [0.3525, 0.4608, 0.2989, 0.4459, 0.7447, 0.0444, 0.1484, 0.4844],\n",
       "         [0.8553, 0.2102, 0.4567, 0.7591, 0.2605, 0.5160, 0.3691, 0.9907]]),\n",
       " tensor([[ 0.0304,  0.0919,  0.9487],\n",
       "         [-0.3488,  0.4826,  0.3844],\n",
       "         [-0.1661, -0.2884,  1.2448]]),\n",
       " tensor([[1.9017, 1.1955, 2.1340, 1.4887, 1.5839, 1.5938, 2.2121, 2.5819],\n",
       "         [1.3747, 1.4796, 0.3333, 1.2542, 1.1961, 1.6838, 1.4372, 2.6818],\n",
       "         [1.4795, 1.4400, 1.4338, 0.9975, 0.5206, 1.4657, 1.9216, 2.1953],\n",
       "         [1.0787, 1.0289, 1.4422, 0.8973, 1.1094, 1.2780, 1.2086, 2.6467],\n",
       "         [0.7267, 0.8122, 1.5535, 1.6860, 0.5672, 1.9600, 1.8566, 2.2734],\n",
       "         [0.5163, 0.5294, 1.0627, 1.4893, 0.5795, 0.5129, 1.3421, 2.6615],\n",
       "         [0.4036, 0.7765, 1.4077, 1.0043, 0.8410, 0.7108, 2.2215, 2.5170],\n",
       "         [1.4351, 1.1057, 1.6938, 1.8541, 0.9737, 1.2580, 1.6438, 2.5218]]),\n",
       " torch.Size([8, 8]),\n",
       " torch.Size([1, 1, 8, 8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(8, 8))\n",
    "K = torch.randn(3,3)\n",
    "conv2d = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,padding=1)\n",
    "X, K, pad_conv2d(1, X, K), pad_conv2d(1, X, K).shape, conv2d(X.reshape(1,1, X.shape[0], X.shape[1])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_stride_conv2d(pad, stride, input, kernel):\n",
    "    if not isinstance(pad, list):\n",
    "        pad = [pad]*2\n",
    "    if not isinstance(stride, list):\n",
    "        stride = [stride]*2\n",
    "        \n",
    "    padded = torch.ones(input.shape[0]+pad[0]*2, input.shape[1]+pad[1]*2)\n",
    "    padded[pad[0]:input.shape[0]+pad[0], pad[1]:input.shape[1]+pad[1]] = input\n",
    "    print(padded.shape)\n",
    "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
    "    out_size = [int((padded.shape[0]-k_h+stride[0])/stride[0]), int((padded.shape[1]-k_w+stride[1])/stride[1])]\n",
    "\n",
    "    output = torch.zeros(out_size, dtype=torch.float32)\n",
    "\n",
    "    for i in range(0,out_size[0]):\n",
    "        for j in range(0,out_size[1]):\n",
    "            h, w = i*stride[0], j*stride[1] \n",
    "            output[i, j] = (padded[h:h+k_h, w:w+k_w] * kernel).sum()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([8, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]), torch.Size([2, 2]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(8, 8))\n",
    "K = torch.rand(3,3)\n",
    "K2 = torch.rand(3,5)\n",
    "pad_stride_conv2d(1, 2, X, K).shape, pad_stride_conv2d([0,1], [3,4], X, K2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 8, 8]), torch.Size([1, 1, 4, 4]), torch.Size([1, 1, 2, 2]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_ps = nn.Conv2d(1, 1, 3, padding=1, stride=2)\n",
    "conv2d_ps2 = nn.Conv2d(in_channels=1,out_channels=1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "X = torch.rand(size=(1, 1, 8, 8))\n",
    "X.shape, conv2d_ps(X).shape, conv2d_ps2(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l] *",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
